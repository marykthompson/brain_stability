{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example scrape segment polarity\n",
    "- Get the segment polarity genes from SBD online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#https://www.tutorialspoint.com/how-can-beautifulsoup-package-be-used-to-parse-data-from-a-webpage-in-python\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError\n",
    "import os\n",
    "import pandas as pd\n",
    "#import sys\n",
    "#sys.path.append('../scripts/')\n",
    "#from plotting_fxns import update_old_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sdbonline.org/sites/fly/aignfam/junction_neuromuscular.htm#Neuromuscular\n",
    "# table_file = '../../resources/neural_loc/nmj_genetable.html'\n",
    "table_file = '../Figures/Examples/segment_polarity.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# columns = soup.findAll('td', text = re.compile('your regex here'), attrs = {'class' : 'pos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/beautifulsoup-find-all-li-in-ul/\n",
    "#Getting the data via Request with 403 error\n",
    "#https://itsmycode.com/python-urllib-error-httperror-http-error-403-forbidden/\n",
    "# HTTPError: HTTP Error 404: Not Found\n",
    "parent_site = 'https://www.sdbonline.org/sites/fly/segment/engrail1.htm'\n",
    "parent_site = 'https://www.sdbonline.org/sites/fly/'\n",
    "soup = BeautifulSoup (open(table_file), \"html.parser\")\n",
    "#https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup\n",
    "tables = soup.find_all('ul')\n",
    "genedict = {}\n",
    "# Beginning of section\n",
    "p = 0\n",
    "for pg in tables:\n",
    "    p += 1\n",
    "    gene_pgs = pg.find_all('a')\n",
    "    for row in gene_pgs:\n",
    "        p+=1\n",
    "        name = row.text\n",
    "        print('name', name)\n",
    "        if row['href'].startswith('http'):\n",
    "            link = row['href']\n",
    "            gene_id = link.split('/')[-1].rstrip('.html')\n",
    "        else:\n",
    "            link = os.path.join(parent_site, row['href'].lstrip('../'))\n",
    "            try:\n",
    "                req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                html = urlopen(req).read()\n",
    "                soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "                result = soup.findAll('href', text=re.compile('FlyBase ID'))\n",
    "                print('result', result)\n",
    "                for a in soup.find_all('a', href=True):\n",
    "                    sourceline = a.sourceline\n",
    "                    if 'flybase.org/reports' in a['href']:\n",
    "                        print('href', a['href'])\n",
    "                        fb_id = a['href'].split('/')[-1].split('.')[0]\n",
    "                        print('fbid', fb_id)\n",
    "            except HTTPError:\n",
    "                print(f'{name} not found')\n",
    "        # hash by FBg ID b/c these will be unique and names might not be unique\n",
    "        for g in gene_ids:\n",
    "            genedict[g] = name\n",
    "        if p>1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/beautifulsoup-find-all-li-in-ul/\n",
    "#Getting the data via Request with 403 error\n",
    "#https://itsmycode.com/python-urllib-error-httperror-http-error-403-forbidden/\n",
    "# HTTPError: HTTP Error 404: Not Found\n",
    "parent_site = 'https://www.sdbonline.org/sites/fly/segment/engrail1.htm'\n",
    "\n",
    "\n",
    "parent_site = 'https://www.sdbonline.org/sites/fly/'\n",
    "soup = BeautifulSoup (open(table_file), \"html.parser\")\n",
    "#https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup\n",
    "tables = soup.find_all('ul')\n",
    "genedict = {}\n",
    "# Beginning of section\n",
    "p = 0\n",
    "for pg in tables:\n",
    "    p += 1\n",
    "    # print('pg', pg)\n",
    "    gene_pgs = pg.find_all('a')\n",
    "    for row in gene_pgs:\n",
    "        name = row.text\n",
    "        # print('name', name)\n",
    "        # print('row', row)\n",
    "        # print('href', row['href'])\n",
    "        if row['href'].startswith('http'):\n",
    "            link = row['href']\n",
    "            gene_id = link.split('/')[-1].rstrip('.html')\n",
    "            # print('gene', gene)\n",
    "        else:\n",
    "            link = os.path.join(parent_site, row['href'].lstrip('../'))\n",
    "            # print('link', link)\n",
    "            try:\n",
    "                req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                html = urlopen(req).read()\n",
    "                soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                lines = text.split('\\n')\n",
    "                for l in lines:\n",
    "                    l = l.strip(' ')\n",
    "                    #also possible to get the FlyBase ID via the links\n",
    "                    if l.startswith('FlyBase ID'):\n",
    "                        print(l)\n",
    "                        print(l.split(':')[-1])\n",
    "                        gene_ids = list(map(lambda x: x.strip(), l.split(':')[-1].split('and')))\n",
    "                        print('gene_ids', gene_ids)\n",
    "                    if l.startswith('Symbol'):\n",
    "                        symbol = l.split('-')[-1].strip()\n",
    "                        print('symbol', symbol)\n",
    "            except HTTPError:\n",
    "                print(f'{name} not found')\n",
    "        # hash by FBg ID b/c these will be unique and names might not be unique\n",
    "        for g in gene_ids:\n",
    "            genedict[g] = name\n",
    "        if p>1:\n",
    "            break\n",
    "        # else:\n",
    "        #     try:\n",
    "        #         link = os.path.join(parent_site, row['href'].lstrip('../'))\n",
    "        #         html = urlopen(link).read()\n",
    "        #         soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "        #         text = soup.get_text()\n",
    "        #         lines = text.split('\\n')\n",
    "        #         for l in lines:\n",
    "        #             l = l.strip(' ')\n",
    "        #             #also possible to get the FlyBase ID via the links\n",
    "        #             if l.startswith('FlyBase ID'):\n",
    "        #                 gene = l.split(':')[1].strip(' ')\n",
    "            # except:\n",
    "            #     print('error on gene %s' % name)\n",
    "            #     continue\n",
    "#     for row in tables[pg].find_all('a'):\n",
    "#         p +=1\n",
    "#         print(p)\n",
    "#         genename = row.text\n",
    "#         if row['href'].startswith('http'):\n",
    "#             link = row['href']\n",
    "#             gene = link.split('/')[-1].rstrip('.html')\n",
    "#         else:\n",
    "#             try:\n",
    "#                 link = os.path.join(parent_site, row['href'].lstrip('../'))\n",
    "#                 html = urlopen(link).read()\n",
    "#                 soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "#                 text = soup.get_text()\n",
    "#                 lines = text.split('\\n')\n",
    "#                 for l in lines:\n",
    "#                     l = l.strip(' ')\n",
    "#                     #also possible to get the FlyBase ID via the links\n",
    "#                     if l.startswith('FlyBase ID'):\n",
    "#                         gene = l.split(':')[1].strip(' ')\n",
    "#             except:\n",
    "#                 print('error on gene %s' % genename)\n",
    "#                 continue\n",
    "# genedict[genename] = gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/beautifulsoup-find-all-li-in-ul/\n",
    "parent_site = 'https://www.sdbonline.org/sites/fly/'\n",
    "soup = BeautifulSoup (open(table_file), \"html.parser\")\n",
    "#https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup\n",
    "tables = soup.find_all('ul')\n",
    "#presynaptic => tables[1]\n",
    "#presynaptc (continued) => tables[3]\n",
    "#postsynaptic => tables[5]\n",
    "#Others => tables[6]\n",
    "##for li in tables[5].find_all('li'):\n",
    "\n",
    "print(tables)\n",
    "# Beginning of section\n",
    "# loc_dict = {'presynaptic':[1, 3], 'postsynaptic':[5], 'others':[6]}\n",
    "# allgenes_dict = {k:{} for k,v in loc_dict.items()}\n",
    "# for loc in loc_dict:\n",
    "#     genedict = {}\n",
    "#     p = 0\n",
    "#     for pg in loc_dict[loc]:\n",
    "#         for row in tables[pg].find_all('a'):\n",
    "#             p +=1\n",
    "#             print(p)\n",
    "#             genename = row.text\n",
    "#             if row['href'].startswith('http'):\n",
    "#                 link = row['href']\n",
    "#                 gene = link.split('/')[-1].rstrip('.html')\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     link = os.path.join(parent_site, row['href'].lstrip('../'))\n",
    "#                     html = urlopen(link).read()\n",
    "#                     soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "#                     text = soup.get_text()\n",
    "#                     lines = text.split('\\n')\n",
    "#                     for l in lines:\n",
    "#                         l = l.strip(' ')\n",
    "#                         #also possible to get the FlyBase ID via the links\n",
    "#                         if l.startswith('FlyBase ID'):\n",
    "#                             gene = l.split(':')[1].strip(' ')\n",
    "#                 except:\n",
    "#                     print('error on gene %s' % genename)\n",
    "#                     continue\n",
    "#             genedict[genename] = gene\n",
    "#     allgenes_dict[loc].update(genedict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pretty')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdb49103a4ed208a05ea4530afbe53462c06fafea10c7833b005d674746fdb08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
